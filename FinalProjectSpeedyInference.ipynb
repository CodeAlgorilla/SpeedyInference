{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNJHg+ha0tlT1wPsNgebzzt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Mount Drive"
      ],
      "metadata": {
        "id": "b4pueZw0BCO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the steps to create environment.\n",
        "1. start google colab and then open this notebook.\n",
        "2. clone the github codebase and pull the newest commits\n",
        "3. install environment\n",
        "4. run notebook in colab with GPU support."
      ],
      "metadata": {
        "id": "ZHxPZ_Y9VD6p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHQ3h_aLAuEk"
      },
      "outputs": [],
      "source": [
        "# step2. clone github code base and pull\n",
        "\n",
        "!git clone https://github.com/CodeAlgorilla/SpeedyInference.git\n",
        "\n",
        "# checkout file directory\n",
        "%cd '/content/SpeedyInference'\n",
        "%mkdir 'logs'\n",
        "\n",
        "!git checkout feature/colab_notebook  # checkout to your branch.\n",
        "!git pull --rebase origin feature/colab_notebook # pull newest changes from your branch.\n",
        "!git status"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "environment setup"
      ],
      "metadata": {
        "id": "-wOaUyZfKjqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install python 3.10 for environment\n",
        "!sudo apt-get install python3.10\n",
        "\n",
        "# install dependencies\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "_1ndoAmPDOOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import python modules and login huggingface with token."
      ],
      "metadata": {
        "id": "h2Hpu7aDMfg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "8Z9Jt2YXMYoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check devices"
      ],
      "metadata": {
        "id": "f1rFh4h4K1et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check device availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"You are using device: %s\" % device)\n",
        "\n",
        "!cat /proc/cpuinfo | grep 'model name'"
      ],
      "metadata": {
        "id": "m0uBaqfxCMeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%pwd # verify the path should be /content/SpeedyInference\n",
        "\n",
        "# Or you can run this command in terminal. Terminal button at Left lower corner.\n",
        "#!torchrun generate.py --model facebook/layerskip-llama2-7B --generation_strategy self_speculative --exit_layer 6 --num_speculations 4"
      ],
      "metadata": {
        "id": "8VIWWeFJK0bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to use baseline model. Change facebook/layerskip-llama2-7B to meta-llama/Llama-2-7b-hf"
      ],
      "metadata": {
        "id": "q2B2FX9BYYvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN-DM Early Exit E-8 in table3\n",
        "!torchrun benchmark.py --model facebook/layerskip-llama2-7B \\\n",
        "    --dataset cnn_dm_summarization \\\n",
        "    --n_shot 1 \\\n",
        "    --num_samples 100 \\\n",
        "    --sample False\\\n",
        "    --generation_strategy autoregressive \\\n",
        "    --exit_layer 8 \\\n",
        "    --output_dir ./logs"
      ],
      "metadata": {
        "id": "UmIpbunZ-O62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN-DM Self Speculation E-8 d-12 in table3\n",
        "!torchrun benchmark.py --model facebook/layerskip-llama2-7B \\\n",
        "    --dataset cnn_dm_summarization \\\n",
        "    --n_shot 1 \\\n",
        "    --num_samples 100 \\\n",
        "    --sample False\\\n",
        "    --generation_strategy self_speculative \\\n",
        "    --exit_layer 8 \\\n",
        "    --num_speculations 12 \\\n",
        "    --output_dir ./logs"
      ],
      "metadata": {
        "id": "FSx3XPix61uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XSUM Early Exit E-8 in table3\n",
        "!torchrun benchmark.py --model facebook/layerskip-llama2-7B \\\n",
        "    --dataset xsum_summarization \\\n",
        "    --n_shot 0 \\\n",
        "    --num_samples 100 \\\n",
        "    --sample False\\\n",
        "    --generation_strategy autoregressive \\\n",
        "    --exit_layer 8 \\\n",
        "    --output_dir ./logs"
      ],
      "metadata": {
        "id": "uM4VZS4jUOlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XSUM Self Speculation E-8 d-12 in table3\n",
        "!torchrun benchmark.py --model facebook/layerskip-llama2-7B \\\n",
        "    --dataset xsum_summarization \\\n",
        "    --n_shot 0 \\\n",
        "    --num_samples 100 \\\n",
        "    --sample False\\\n",
        "    --generation_strategy self_speculative \\\n",
        "    --exit_layer 8 \\\n",
        "    --num_speculations 12 \\\n",
        "    --output_dir ./logs"
      ],
      "metadata": {
        "id": "ltxLprTeUOlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HumanEval Early Exit E-8 in table3\n",
        "!torchrun benchmark.py --model facebook/layerskip-llama2-7B \\\n",
        "    --dataset human_eval \\\n",
        "    --n_shot 0 \\\n",
        "    --num_samples 100 \\\n",
        "    --sample False\\\n",
        "    --generation_strategy autoregressive \\\n",
        "    --exit_layer 8 \\\n",
        "    --output_dir ./logs"
      ],
      "metadata": {
        "id": "dRnQJbq-UPWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HumanEval Self Speculation E-8 d-12 in table3\n",
        "!torchrun benchmark.py --model facebook/layerskip-llama2-7B \\\n",
        "    --dataset human_eval \\\n",
        "    --n_shot 0 \\\n",
        "    --num_samples 100 \\\n",
        "    --sample False\\\n",
        "    --generation_strategy self_speculative \\\n",
        "    --exit_layer 8 \\\n",
        "    --num_speculations 6 \\\n",
        "    --output_dir ./logs"
      ],
      "metadata": {
        "id": "8nFoTq1nUPWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Exit E-6 in table 5\n",
        "!torchrun benchmark.py --model facebook/layerskip-codellama-7B \\\n",
        "    --dataset human_eval \\\n",
        "    --n_shot 0 \\\n",
        "    --num_samples 100 \\\n",
        "    --sample False\\\n",
        "    --generation_strategy autoregressive \\\n",
        "    --exit_layer 6 \\\n",
        "    --output_dir ./logs"
      ],
      "metadata": {
        "id": "Uf1301kj_gfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Exit E-6 Speculation-12 in table 5\n",
        "!torchrun benchmark.py --model facebook/layerskip-codellama-7B \\\n",
        "    --dataset human_eval \\\n",
        "    --n_shot 0 \\\n",
        "    --num_samples 100 \\\n",
        "    --sample False\\\n",
        "    --generation_strategy self_speculative \\\n",
        "    --exit_layer 6 \\\n",
        "    --num_speculations 12 \\\n",
        "    --output_dir ./logs"
      ],
      "metadata": {
        "id": "GQDSt_ijB3we"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}