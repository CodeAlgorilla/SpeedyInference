{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOb4AdRZGLjP+euC83fSBXz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Mount Drive"
      ],
      "metadata": {
        "id": "b4pueZw0BCO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the steps to create environment.\n",
        "1. start google colab and then open this notebook.\n",
        "2. clone the github codebase and pull the newest commits\n",
        "3. install environment\n",
        "4. run notebook in colab with GPU support."
      ],
      "metadata": {
        "id": "ZHxPZ_Y9VD6p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHQ3h_aLAuEk"
      },
      "outputs": [],
      "source": [
        "# step2. clone github code base and pull\n",
        "\n",
        "!git clone https://github.com/CodeAlgorilla/SpeedyInference.git\n",
        "\n",
        "# checkout file directory\n",
        "%cd '/content/SpeedyInference'\n",
        "%mkdir 'logs'\n",
        "\n",
        "!git checkout feature/colab_notebook  # checkout to your branch.\n",
        "!git pull --rebase origin feature/colab_notebook # pull newest changes from your branch.\n",
        "!git status"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "environment setup"
      ],
      "metadata": {
        "id": "-wOaUyZfKjqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install python 3.10 for environment\n",
        "!sudo apt-get install python3.10\n",
        "\n",
        "# install dependencies\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "_1ndoAmPDOOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import python modules and login huggingface with token."
      ],
      "metadata": {
        "id": "h2Hpu7aDMfg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "8Z9Jt2YXMYoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check devices"
      ],
      "metadata": {
        "id": "f1rFh4h4K1et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check device availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"You are using device: %s\" % device)\n",
        "\n",
        "!cat /proc/cpuinfo | grep 'model name'"
      ],
      "metadata": {
        "id": "m0uBaqfxCMeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd # verify the path should be /content/SpeedyInference\n",
        "\n",
        "# Or you can run this command in terminal. Terminal button at Left lower corner.\n",
        "!torchrun generate.py --model facebook/layerskip-llama2-7B --generation_strategy self_speculative --exit_layer 6 --num_speculations 4"
      ],
      "metadata": {
        "id": "8VIWWeFJK0bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xty4N-3bUNon"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}