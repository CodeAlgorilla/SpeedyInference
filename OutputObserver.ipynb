{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeAlgorilla/SpeedyInference/blob/feature%2Fcolab_notebook/OutputObserver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbGV_TOiLDx6"
      },
      "source": [
        "Created by: [Mostafa Elhoushi](https://huggingface.co/melhoushi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBRONCOasAA_"
      },
      "source": [
        "## Install requirements\n",
        "\n",
        "First, run the cells below to install the requirements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLE3fo0wJ7kk"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOrV6rbBuXiP"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypFy9WpHDvUp"
      },
      "source": [
        "Now, let's import some libraries and classes that we will need in this demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qob6JMjPDk4d"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "\n",
        "orig_model = None\n",
        "layerskip_model = None\n",
        "orig_tokenizer = None\n",
        "layerskip_tokenizer = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcSwzB9mHGRU"
      },
      "source": [
        "print function for all outputs at each layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaKxCk3vYZA0"
      },
      "outputs": [],
      "source": [
        "def get_early_exit_predictions(hidden_states, lm_head, tokenizer):\n",
        "  layer2text = dict()\n",
        "  device = hidden_states[0][0].device\n",
        "  for layer_idx in range(len(hidden_states[0])):\n",
        "    output_ids = torch.empty((1,1), device=device, dtype=torch.int)\n",
        "    for token_idx in range(len(hidden_states)):\n",
        "      logits = lm_head(hidden_states[token_idx][layer_idx])\n",
        "      probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "      out = torch.argmax(probs, dim=-1)\n",
        "      output_ids = torch.cat((output_ids, out), dim=-1)\n",
        "    text = tokenizer.batch_decode(output_ids[0])\n",
        "    layer2text[layer_idx] = text\n",
        "  return layer2text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwT0M8CPHGRU"
      },
      "source": [
        "And let's set some default generation configuration for this demo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J5WpqoUziC-"
      },
      "outputs": [],
      "source": [
        "generation_config = {\n",
        "    \"do_sample\": False,\n",
        "    \"temperature\": None,\n",
        "    \"top_p\": None,\n",
        "    \"max_new_tokens\": 12\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nipj233gHGRU"
      },
      "source": [
        "Choose the origin model ad the layerskip model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt28uLMSLJhJ"
      },
      "outputs": [],
      "source": [
        "orig_checkpoint = \"meta-llama/Llama-3.2-1B\"         # meta-llama/Llama-2-7b-hf    meta-llama/Llama-2-13b-hf    meta-llama/Meta-Llama-3-8B      meta-llama/Llama-3.2-1B      meta-llama/CodeLlama-7b-hf\n",
        "layerskip_checkpoint = \"facebook/layerskip-llama3.2-1B\"  # facebook/layerskip-llama2-7B   facebook/layerskip-llama2-13B  facebook/layerskip-llama3-8B     facebook/layerskip-llama3.2-1B  facebook/layerskip-codellama-7B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OBlXqveLTVz"
      },
      "source": [
        "Load original model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvB-fY8CpUaE"
      },
      "outputs": [],
      "source": [
        "orig_model = AutoModelForCausalLM.from_pretrained(\n",
        "    orig_checkpoint,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "orig_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    orig_checkpoint\n",
        ")\n",
        "\n",
        "orig_model.generation_config.pad_token_id = orig_tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(orig_model)\n"
      ],
      "metadata": {
        "id": "4ZeoEiOtsQzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEWK4luHHGRV"
      },
      "source": [
        "Load layerskip model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "781kLYAZCiPg"
      },
      "outputs": [],
      "source": [
        "layerskip_model = AutoModelForCausalLM.from_pretrained(\n",
        "    layerskip_checkpoint,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "layerskip_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    layerskip_checkpoint\n",
        ")\n",
        "\n",
        "layerskip_model.generation_config.pad_token_id = layerskip_tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(layerskip_model)"
      ],
      "metadata": {
        "id": "NUo4Wu0mKYmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "delete model"
      ],
      "metadata": {
        "id": "XCqJ36R_J5Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if orig_model is not None:\n",
        "  del orig_model\n",
        "  del orig_tokenizer\n",
        "  orig_model = None\n",
        "  orig_tokenizer = None\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "if layerskip_model is not None:\n",
        "  del layerskip_model\n",
        "  del layerskip_tokenizer\n",
        "  layerskip_model = None\n",
        "  layerskip_tokenizer = None\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "1aVoRpYWJ4LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3cue_j_D7SB"
      },
      "source": [
        "Let's create a prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNNPn8BvEhCe"
      },
      "outputs": [],
      "source": [
        "prompt = \"Once upon a time\"\n",
        "code_prompt = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clQmCZtXHGRV"
      },
      "source": [
        "Generate tokens using origin model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0rNxrtR3Zk2"
      },
      "outputs": [],
      "source": [
        "orig_inputs = orig_tokenizer(prompt, return_tensors=\"pt\").to(orig_model.device)\n",
        "\n",
        "\n",
        "orig_outputs = orig_model.generate(\n",
        "    **orig_inputs,\n",
        "    **generation_config,\n",
        "    return_dict_in_generate=True,\n",
        "    output_hidden_states=True\n",
        ")\n",
        "#print(orig_outputs)\n",
        "\n",
        "# Convert Output Token IDs to Output Text\n",
        "orig_text = orig_tokenizer.decode(orig_outputs[\"sequences\"][0], skip_special_tokens=True)\n",
        "print(orig_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLANigFqHGRV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# check hidden states\n",
        "hidden_states = orig_outputs[\"hidden_states\"]\n",
        "print(\"hidden_states:\")\n",
        "\n",
        "print(f\"  type(hidden_states): {type(hidden_states)}\")\n",
        "print(f\"  len(hidden_states): {len(hidden_states)}\")\n",
        "\n",
        "print(f\"    type(hidden_states[0]): {type(hidden_states[0])}\")\n",
        "print(f\"    len(hidden_states[0]): {len(hidden_states[0])}\")\n",
        "\n",
        "print(f\"      type(hidden_states[0][0]): {type(hidden_states[0][0])}\")\n",
        "print(f\"      hidden_states[0][0].shape: {hidden_states[0][0].shape}\")\n",
        "print(f\"      hidden_states[1][0].shape: {hidden_states[1][0].shape}\")\n",
        "\n",
        "batch_size, input_seq_len = orig_inputs[\"input_ids\"].shape\n",
        "batch_size, total_seq_len = orig_outputs[\"sequences\"].shape\n",
        "\n",
        "prompt_len = input_seq_len\n",
        "num_steps = total_seq_len - input_seq_len\n",
        "emb_dim = orig_model.config.hidden_size\n",
        "num_layers = len(orig_model.model.layers)\n",
        "\n",
        "print(f\"batch_size: {batch_size}\\n\"\n",
        "      f\"prompt_len: {prompt_len}\\n\"\n",
        "      f\"num_steps: {num_steps}\\n\"\n",
        "      f\"emb_dim: {emb_dim}\\n\"\n",
        "      f\"num_layers: {num_layers}\")\n",
        "\n",
        "assert(len(hidden_states) == num_steps)\n",
        "assert(len(hidden_states[0]) == num_layers + 1) # add 1 to count embedding layer\n",
        "# Tensors of step 0 process prompt\n",
        "assert(hidden_states[0][0].shape == (batch_size, prompt_len, emb_dim))\n",
        "# Tensors of each remaining step processes a single token\n",
        "assert(hidden_states[1][0].shape == (batch_size, 1, emb_dim))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr8xr3qEklCd"
      },
      "source": [
        "Now, let's print a table that shows the full predicted text when exiting at each layer in origin model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXf3wSEadmJJ"
      },
      "outputs": [],
      "source": [
        "batch_size, input_seq_len = orig_inputs[\"input_ids\"].shape\n",
        "batch_size, total_seq_len = orig_outputs[\"sequences\"].shape\n",
        "\n",
        "orig_layer_2_text = get_early_exit_predictions(\n",
        "    orig_outputs[\"hidden_states\"],\n",
        "    orig_model.lm_head,\n",
        "    orig_tokenizer\n",
        ")\n",
        "\n",
        "orig_df = pd.DataFrame.from_dict(orig_layer_2_text, orient=\"index\", columns=np.arange(total_seq_len))\n",
        "orig_df = orig_df.replace({\"\\n\": r\"\\textbackslash n\", \"#\": r\"\\#\"}, regex=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVcIuNl7d7PF"
      },
      "outputs": [],
      "source": [
        "orig_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export data to LaTex"
      ],
      "metadata": {
        "id": "c0mPI3JrL8NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orig_df_latex = orig_df.to_latex(escape=False)\n",
        "orig_df_latex\n",
        "with open('original_output_table.tex', 'w') as f:\n",
        "    f.write(orig_df_latex)"
      ],
      "metadata": {
        "id": "T0nd9LC1L6lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATWq4PzKHGRV"
      },
      "source": [
        "Show the full predicted text when exiting at each layer in a layerskip model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOfeAV9htaB5"
      },
      "outputs": [],
      "source": [
        "layerskip_inputs = layerskip_tokenizer(prompt, return_tensors=\"pt\").to(layerskip_model.device)\n",
        "\n",
        "layerskip_outputs = layerskip_model.generate(\n",
        "    **layerskip_inputs,\n",
        "    **generation_config,\n",
        "    return_dict_in_generate=True,\n",
        "    output_hidden_states=True\n",
        ")\n",
        "\n",
        "layerskip_layer_2_text = get_early_exit_predictions(\n",
        "    layerskip_outputs[\"hidden_states\"],\n",
        "    layerskip_model.lm_head,\n",
        "    layerskip_tokenizer\n",
        ")\n",
        "\n",
        "layerskip_text = layerskip_tokenizer.decode(layerskip_outputs[\"sequences\"][0], skip_special_tokens=True)\n",
        "print(layerskip_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhePh8PfuAPr"
      },
      "outputs": [],
      "source": [
        "batch_size, input_seq_len = layerskip_inputs[\"input_ids\"].shape\n",
        "batch_size, total_seq_len = layerskip_outputs[\"sequences\"].shape\n",
        "\n",
        "layerskip_df = pd.DataFrame.from_dict(layerskip_layer_2_text, orient=\"index\", columns=np.arange(total_seq_len))\n",
        "#layerskip_df = layerskip_df.style.set_properties(**{'text-align': 'left'})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print and export output"
      ],
      "metadata": {
        "id": "rHzw68tC74aF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRJ-l7bxuFlY"
      },
      "outputs": [],
      "source": [
        "layerskip_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layerskip_df = layerskip_df.replace({\"\\n\": r\"\\textbackslash n\", \"#\": r\"\\#\"}, regex=True)\n",
        "layer_skip_latex_code = layerskip_df.to_latex(escape=False)\n",
        "\n",
        "layer_skip_latex_code\n",
        "with open('layerskip_output_table.tex', 'w') as f:\n",
        "    f.write(layer_skip_latex_code)"
      ],
      "metadata": {
        "id": "yPVENdReQU8P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}